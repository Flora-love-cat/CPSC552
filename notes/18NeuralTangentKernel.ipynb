{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Tangent Kernel (NTK)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NTK = infinite width NN\n",
    "\n",
    "- Neural networks generally perform better than NTKs even at “infinite width limit”\n",
    "\n",
    "- NTK = kernel ridge regression (ridge regression + kernel trick)\n",
    "\n",
    "- SGD of NTK = kernel gradient descent = solve under-determined linear system (more parameters than constraints)\n",
    "\n",
    "    generalization: low norm initialization leads to a global minima which is minimal norm solution\n",
    "\n",
    "- **Why use NTK?** \n",
    "\n",
    "    Provides insights into neural network convergence, generalization, and the relationship between kernel methods and deep learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NTK matrix is a symmetric matrix $K \\in \\mathbb{R}^{n \\times n}$\n",
    "\n",
    "$$\n",
    "K=\\Phi^T\\Phi= [\\nabla_{\\theta} f(\\theta_0)]^T \\nabla_{\\theta} f(\\theta_0)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel function: innerproduct of gradients of the neural network's output w.r.t initial network parameters $\\theta$ at input points $\\mathbf{x}_i$ and $\\mathbf{x}_j$\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\langle \\nabla_{\\theta} f(\\mathbf{x}_i, \\theta_0), \\nabla_{\\theta} f(\\mathbf{x}_j, \\theta_0) \\rangle\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kernel regression is a Non-parametric model that relies on a kernel function to find a non-linear relation between a pair of random variables $X$ and $Y$.\n",
    "\n",
    "- predicted value of function $m$ at point $\\mathbf{x}$ is estimated as a local weighted average\n",
    "\n",
    "  $$\\hat{m}_h(\\mathbf{x}) = \\frac{\\sum_{i=1}^{n} K_h(\\mathbf{x}, \\mathbf{x}_i) y_i}{\\sum_{i=1}^{n} K_h(\\mathbf{x}, \\mathbf{x}_i)}$$\n",
    "\n",
    "  where weighting function is kernel $K_h(\\mathbf{x}, \\mathbf{x}_i)$ measuring similarity between $\\mathbf{x}$ and $\\mathbf{x}_i$, \n",
    "  \n",
    "  $h$ is bandwidth, the only parameter of kernel function\n",
    "\n",
    "  $y_i$ is the target value for $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Kernel Regression:**\n",
    "\n",
    "  - Flexibility in modeling complex non-linear relationship between variables, as the kernel function can adapt to different data structures.\n",
    "\n",
    "  - Easy interpretation, as it provides a smooth function that follows the trend of data.\n",
    "\n",
    "  - Robust to outliers, as the kernel function can down-weight the influence of distant points.\n",
    "\n",
    "  - Requires fewer assumptions about the data compared to parametric methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lazy regime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a NN with very large width or highly scaled activations can lead to lazy regime.\n",
    "\n",
    "lazy regime refers to a training regime when network is initialized with small weights, and learning rate is low enough, the weights of NN change only slightly during training and Jacobian matrix remains almost constant.\n",
    "\n",
    "NTK matrix becomes a good linear approximation of the learning dynamics.\n",
    " \n",
    "This results in a simpler model with a smooth output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradient flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "derive gradient flow for a loss function $L$ and a neural network $f(\\mathbf{x}, \\theta)$ with respect to the network's parameters $\\theta$. \n",
    "\n",
    "1. Start with the gradient of the loss function with respect to the network's output:\n",
    "\n",
    "   $$\\nabla_{\\mathbf{f}} L(\\mathbf{f}(\\mathbf{x}, \\theta))$$\n",
    "\n",
    "2. Apply the chain rule to compute the gradient of the loss function with respect to the network's parameters:\n",
    "\n",
    "   $$\\nabla_{\\theta} L(\\mathbf{f}(\\mathbf{x}, \\theta)) = \\nabla_{\\mathbf{f}} L(\\mathbf{f}(\\mathbf{x}, \\theta)) \\cdot \\nabla_{\\theta} \\mathbf{f}(\\mathbf{x}, \\theta)$$\n",
    "\n",
    "3. Define gradient flow as the continuous-time limit of gradient descent with infinitesimal learning rate $\\eta$:\n",
    "\n",
    "   $$\\frac{d\\theta(t)}{dt} = -\\eta \\nabla_{\\theta} L(\\mathbf{f}(\\mathbf{x}, t))$$\n",
    "\n",
    "4. Substitute the chain rule result from step 2 into the gradient flow equation:\n",
    "\n",
    "   $$\\frac{d\\theta(t)}{dt} = -\\eta \\nabla_{\\mathbf{f}} L(\\mathbf{f}(\\mathbf{x}, t)) \\cdot \\nabla_{\\theta} \\mathbf{f}(\\mathbf{x}, t)$$\n",
    "\n",
    "5. Since we're interested in the dynamics of the network's output, apply the chain rule again to obtain the gradient flow of the output:\n",
    "\n",
    "   $$\\frac{d\\mathbf{f}(\\mathbf{x}, t)}{dt} = \\frac{d\\mathbf{f}(\\mathbf{x}, t)}{d\\theta(t)} \\cdot \\frac{d\\theta(t)}{dt}$$\n",
    "\n",
    "6. Substitute the gradient flow of the parameters from step 4:\n",
    "\n",
    "   $$\\frac{d\\mathbf{f}(\\mathbf{x}, t)}{dt} = \\nabla_{\\theta} \\mathbf{f}(\\mathbf{x}, t) \\cdot (-\\eta \\nabla_{\\mathbf{f}} L(\\mathbf{f}(\\mathbf{x}, t)) \\cdot \\nabla_{\\theta} \\mathbf{f}(\\mathbf{x}, t))$$\n",
    "\n",
    "7. In the lazy regime, the Jacobian $\\nabla_{\\theta} \\mathbf{f}(\\mathbf{x}, t)$ remains almost constant, so the NTK matrix $\\Theta(t)$ can be used to approximate the learning dynamics:\n",
    "\n",
    "   $$\\frac{d\\mathbf{f}(\\mathbf{x}, t)}{dt} \\approx -\\Theta(t) \\nabla_{\\mathbf{f}} L(\\mathbf{f}(\\mathbf{x}, t))$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel gradient descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "the gradient in the directions associated with larger eigenvalues will decay faster than those associated with smaller eigenvalues, because these directions capture the most significant variations in the loss landscape."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel gradient descent is a variant of gradient descent that uses the Neural Tangent Kernel (NTK) to guide the optimization process. \n",
    "\n",
    "We derive the kernel gradient descent by considering the descent along eigenvectors of the NTK matrix.\n",
    "\n",
    "1. Start with the gradient flow equation in the lazy regime:\n",
    "\n",
    "   $$\\frac{d\\mathbf{f}(\\mathbf{x}, t)}{dt} = -\\Theta(t) \\nabla_{\\mathbf{f}} L(\\mathbf{f}(\\mathbf{x}, t))$$\n",
    "\n",
    "2. Decompose the NTK matrix $\\Theta(t)$ using its eigenvectors and eigenvalues:\n",
    "\n",
    "   $$\\Theta(t) = \\sum_{i=1}^{n} \\lambda_i(t) \\mathbf{v}_i(t) \\mathbf{v}_i^T(t)$$\n",
    "\n",
    "   Here, $\\lambda_i(t)$ and $\\mathbf{v}_i(t)$ are the eigenvalues and eigenvectors of the NTK matrix, respectively.\n",
    "\n",
    "3. Project the gradient of the loss function onto the eigenvectors of the NTK matrix:\n",
    "\n",
    "   $$\\nabla_{\\mathbf{f}} L(\\mathbf{f}(\\mathbf{x}, t)) = \\sum_{i=1}^{n} \\alpha_i(t) \\mathbf{v}_i(t)$$\n",
    "\n",
    "   where $\\alpha_i(t) = \\mathbf{v}_i^T(t) \\nabla_{\\mathbf{f}} L(\\mathbf{f}(\\mathbf{x}, t))$ are the coefficients of the projection.\n",
    "\n",
    "4. Substitute the eigendecomposition of the NTK matrix and the projection of the gradient into the gradient flow equation:\n",
    "\n",
    "   $$\\frac{d\\mathbf{f}(\\mathbf{x}, t)}{dt} = -\\sum_{i=1}^{n} \\lambda_i(t) \\mathbf{v}_i(t) \\mathbf{v}_i^T(t) \\sum_{j=1}^{n} \\alpha_j(t) \\mathbf{v}_j(t)$$\n",
    "\n",
    "5. Compute the dot product between eigenvectors and rearrange the summation:\n",
    "\n",
    "   $$\\frac{d\\mathbf{f}(\\mathbf{x}, t)}{dt} = -\\sum_{i=1}^{n} \\lambda_i(t) \\alpha_i(t) \\mathbf{v}_i(t)$$\n",
    "\n",
    "6. The kernel gradient descent update rule for the network's output is then given by:\n",
    "\n",
    "   $$\\mathbf{f}(\\mathbf{x}, t + \\Delta t) = \\mathbf{f}(\\mathbf{x}, t) - \\eta \\sum_{i=1}^{n} \\lambda_i(t) \\alpha_i(t) \\mathbf{v}_i(t)$$\n",
    "\n",
    "   where $\\eta$ is the learning rate and $\\Delta t$ is the time step.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
