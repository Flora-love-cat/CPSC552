{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural ODE Net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://m0nads.files.wordpress.com/2019/06/rect1494.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image \n",
    "url='https://m0nads.files.wordpress.com/2019/06/rect1494.png'\n",
    "Image(url=url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traditional deep learning models use discrete layers and fixed architectures\n",
    "    \n",
    "    e.g., **ResNet**: uses skip connections to mitigate vanishing and exploding gradients in deep neural networks.\n",
    "\n",
    "\n",
    "- Neural ODE Net is a new type of **infinite deep** NN based on ODE that provide a continuous-time alternative for modeling complex systems\n",
    "\n",
    "    2-4 x depth of RedNet\n",
    "\n",
    "\n",
    "- **Dynamic system**: A system whose behavior changes over time based on a set of rules, often modeled using ODEs.\n",
    "\n",
    "- **Ordinary Differential Equation (ODE)**: A mathematical equation that describes the relationship between a function $z$ and its derivatives $f$, often used to model dynamic systems.\n",
    "\n",
    "    $$\n",
    "    \\frac{dz(t)}{dt} = f(z(t), t)\\\\[1em]\n",
    "    \\text{Initial condition:\\ } z(t_0) = z_0\n",
    "    $$\n",
    "\n",
    "- Initial Value Problem:\n",
    "\n",
    "    Given an ODE $\\frac{d\\mathbf{z}(t)}{dt} = f(\\mathbf{z}(t), t; \\theta)$ with initial condition $\\mathbf{z}(t_0) = \\mathbf{z}_0$,\n",
    "\n",
    "    the goal is to find the function $\\mathbf{z}(t)$ that satisfies the ODE and the initial condition.\n",
    "\n",
    "- **Euler Integrators**: A family of numerical methods for solving ODEs by approximating the solution at discrete time steps $t_0, t_1, t_2, \\dots$ with small Step size: $h$, The update rule is: \n",
    "\n",
    "    $$z(t+h) = z(t) + hf(z, t)$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm for training the latent ODE model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the latent representation $\\mathbf{z}_{t_0}$ (initial state of the system at time $t_0$), we traverse the sequence using RNN and obtain parameters $\\phi$ of variational distribution $q(\\mathbf{z}_{t_0}|\\{\\mathbf{x}_{t_i}, t_i\\}_i, \\phi)$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LRtGTST73BgRE33BgXS2AA.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image \n",
    "url='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LRtGTST73BgRE33BgXS2AA.png'\n",
    "Image(url=url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm follows a standard VAE algorithm with an RNN variational posterior and an ODESolve model:\n",
    "\n",
    "1. Run an RNN encoder through the time series and infer the parameters for a posterior over $\\mathbf{z}_{t_0}$: \n",
    "\n",
    "$$q(\\mathbf{z}_{t_0}|\\{\\mathbf{x}_{t_i}, t_i\\}_i, \\phi) = \\mathcal{N}(\\mathbf{z}_{t_0}|\\mu_{\\mathbf{z}_{t_0}}, \\sigma_{\\mathbf{z}_0})$$\n",
    "\n",
    "- $\\mu_{\\mathbf{z}_{t_0}}, \\sigma_{\\mathbf{z}_0}$ are mean and standard deviation comes from the hidden state $\\mathbf{h}_{t_0}, ..., \\mathbf{h}_{t_N}$ of $\\text{RNN}(\\{\\mathbf{x}_{t_i}, t_i\\}_i, \\phi)$\n",
    "\n",
    "- $\\{\\mathbf{x}_{t_i}, t_i\\}_i$: input sequence with observations $\\mathbf{x}_{t_i}$ and corresponding times $t_i$\n",
    "   \n",
    "2. Sample latent representation from posterior distribution\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_{t_0} \\sim q(\\mathbf{z}_{t_0}|\\{\\mathbf{x}_{t_i}, t_i\\}_i)\n",
    "$$\n",
    "\n",
    "3. Obtain $\\mathbf{z}_{t_1}, \\mathbf{z}_{t_2},..., \\mathbf{z}_{t_M}$ by solving ODE \n",
    "\n",
    "$$\\text{ODESolve}(\\mathbf{z}_{t_0}, f, \\theta, t_0,...,t_M)$$\n",
    "\n",
    "- $f$: the neural network representing the ODE dynamics, parameterized by $\\theta$. defining the gradient $d\\mathbf{z}/dt$ as a function of $\\mathbf{z}$\n",
    "\n",
    "- $\\text{ODESolve}$ is a numerical solver that approximates the solution of the ODE. e.g., Runge-Kutta method.\n",
    "\n",
    "\n",
    "4. Maximize evidence lower bound (ELBO), a lower bound on the log-likelihood of the data\n",
    "\n",
    "$$\n",
    "\\text{ELBO} = \\frac{1}{M}\\sum_{i=1}^M \\log p(\\mathbf{x}_{t_i}|\\mathbf{z}_{t_i}, \\theta_x) + \\log p(\\mathbf{z}_{t_0}) - \\log q(\\mathbf{z}_{t_0}|\\{\\mathbf{x}_{t_i}, t_i\\}_i, \\phi)\n",
    "$$\n",
    "\n",
    "- $p(\\mathbf{x}_{t_i}|\\mathbf{z}_{t_i}, \\theta_x)$: likelihood of the observed data $\\mathbf{x}_{t_i}$ given the latent variable $\\mathbf{z}_{t_i}$ and the model parameters $\\theta_x$\n",
    "\n",
    "- $p(\\mathbf{z}_{t_0}) = \\mathcal{N}(0, 1)$: prior distribution over latent variable $\\mathbf{z}_{t_0}$, which is standard normal\n",
    "\n",
    "- $q(\\mathbf{z}_{t_0}|{\\mathbf{x}_{t_i}, t_i}_i, \\phi)$: posterior  distribution over latent variable $\\mathbf{z}_{t_0}$ given the input sequence ${\\mathbf{x}_{t_i}, t_i}_i^M$ and variational parameters $\\phi$\n",
    "\n",
    "- the sum is taken over all time steps $t_1, t_2, ..., t_M$\n",
    "\n",
    "- $M$: length of the time series and determines the granularity of the latent representation.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compute the loss: \n",
    "\n",
    "   $$\n",
    "   L(\\mathbf{z}(t_1)) = L(\\mathbf{z}(t_0) + \\int_{t_0}^{t_1} f(\\mathbf{z}(t), t, \\theta)dt) = L(\\text{ODESolve}(\\mathbf{z}(t_0), f, t_0, t_1, \\theta))\n",
    "   $$\n",
    "\n",
    "- $L$ is negative log likelihood loss function.\n",
    "- $\\mathbf{z}(t_0)$ is the initial state of the system at time $t_0$.\n",
    "- $\\mathbf{z}(t_1)$ is the predicted state of the system at time $t_1$.\n",
    "\n",
    "\n",
    "6. Backward pass: Calculate gradients of loss w.r.t model parameters $\\theta$ and initial state $\\mathbf{z}_0$ using the **adjoint sensitivity** method \n",
    "\n",
    "   - Define the adjoint state as derivative of loss w.r.t hidden state $\\mathbf{z}(t)$ at intermediate timepoint $t$\n",
    "   \n",
    "      $$\\mathbf{a}(t) = \\frac{dL}{d\\mathbf{z}(t)}$$\n",
    "\n",
    "   - Solve the adjoint ODE \n",
    "   \n",
    "      $$\\frac{d\\mathbf{a}(t)}{dt} = -\\mathbf{a}(t)^T \\frac{df(\\mathbf{z}(t), t; \\theta)}{d\\mathbf{z}}\n",
    "      $$ \n",
    "      \n",
    "      backward in time with terminal condition $\\mathbf{a}(t_1) = \\frac{dL}{d\\mathbf{z}(t_1)}$.\n",
    "\n",
    "   - Compute the gradients: \n",
    "   \n",
    "   $$\n",
    "   \\frac{dL}{d\\theta} = \\int_{t_0}^{t_1} \\mathbf{a}(t)^T \\frac{df(\\mathbf{z}(t), t; \\theta)}{d\\theta} dt\\\\[1em]\n",
    "   \\frac{dL}{d\\mathbf{z}_{t_0}} = \\mathbf{a}(t_0)\n",
    "   $$\n",
    "\n",
    "\n",
    "7. Update the parameters: Update the model parameters $\\theta$ using an optimization algorithm (e.g., gradient descent) and the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.researchgate.net/profile/Xiang-Xie/publication/337539890/figure/fig2/AS:872753787846656@1585092126559/depicts-this-unrolled-architecture-of-neural-ODE-based-encoder-decoder-modules-used-for.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://www.researchgate.net/profile/Xiang-Xie/publication/337539890/figure/fig2/AS:872753787846656@1585092126559/depicts-this-unrolled-architecture-of-neural-ODE-based-encoder-decoder-modules-used-for.jpg'\n",
    "Image(url=url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advantage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory-efficient: Neural ODEs use **adjoint sensitivity** method to compute gradient, which don't need directly backpropagating through the ODE solver.\n",
    "\n",
    "Adaptive computation: The ODE solver can adapt its computation to the complexity of the problem, allocating more resources when needed and reducing computations for simpler cases.\n",
    "\n",
    "Continuous-time models: Unlike recurrent neural networks, which require discretizing observation and emission intervals, continuously-defined dynamics can naturally incorporate data\n",
    "which arrives at arbitrary times, which can provide better representations for certain applications, such as time series analysis, physical simulations, or biological systems.\n",
    "\n",
    "\n",
    "Scalable and invertible normalizing flows: change of variables formula becomes easier to compute. we construct a new class of invertible density models that avoids the single-unit bottleneck of normalizing flows, and can be trained directly by maximum likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalizing flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normalizing flow is a **generative model** that transforms a simple base distribution $Q$ (e.g., Gaussian) into a more complex distribution $P$ by applying a series of invertible and differentiable transformations $f$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deep normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Algorithm:\n",
    "\n",
    "1. Sample a batch of points $z_0$ from the base distribution $Q$.\n",
    "\n",
    "2. Apply the series of invertible and differentiable transformations to obtain $z(t)$.\n",
    "\n",
    "    $$z(t_1) = f(z(t_0)) \\Rightarrow z(t_N) = f_N \\circ f_{N-1} \\circ \\cdots \\circ f_1 (z(t_0))$$\n",
    "\n",
    "3. Compute log-likelihood of the transformed points $z(t)$ using **change of variables formula**.\n",
    "\n",
    "  $$\n",
    "  \\log p(z(t_1)) = \\log p(z(t_0)) - \\log \\left| \\det \\frac{\\partial f}{\\partial z(t_0)} \\right| \\Rightarrow\n",
    "  \\log p(z(t_N)) = \\log p(z(t_0)) - \\sum_{n=1}^N\\log \\left| \\det \\frac{\\partial f_n}{\\partial z(t_{n-1})} \\right|\n",
    "  $$\n",
    "\n",
    "4. Maximize log-likelihood w.r.t the parameters of the transformations using an optimization algorithm\n",
    "\n",
    "$$\n",
    "\\max_Q \\mathbb{E}_{P}\\log Q\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## continous normalizing flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Algorithm:\n",
    "\n",
    "1. Sample a batch of points $z_0$ from the base distribution $Q$.\n",
    "\n",
    "2. Apply the series of invertible and differentiable transformations to obtain $z(t)$.\n",
    "\n",
    "    $$\n",
    "    z(t_N) = F(z(t_0)) = \\int_{t_0}^{t_N} f(z(t_N), t)dt\n",
    "    $$\n",
    "\n",
    "3. Compute log-likelihood of the transformed points $z(t)$ using **change of variables formula**.\n",
    "\n",
    "  $$ \n",
    "  \\log p(z(t_N)) = \\log p(z(t_0)) - \\int_{t_0}^{t_N} \\text{Trace}\\left( \\frac{\\partial f}{\\partial z(t)}\\right)dt\n",
    "  $$\n",
    "\n",
    "4. Maximize log-likelihood w.r.t the parameters of the transformations using an optimization algorithm\n",
    "\n",
    "$$\n",
    "\\max_Q \\mathbb{E}_{P}\\log Q\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
